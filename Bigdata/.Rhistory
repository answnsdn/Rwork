filter_content <- paste(filter_content,collapse = "")
filter_content <- gsub("<.*?>","",filter_content)#gsub(지울부분,지운부분 대체할내용,지울 데이터 이름)
filter_content <- gsub("\t|&nbsp;","",filter_content)
filter_content
#기존의 저장되어 있던 contentlist 내용에 추가
contentlist <- c(contentlist,filter_content)
contentlist
cat("\n",page)
final_crawl_data <- cbind(final_title,final_hit, filter_url,contentlist)
}
final_crawl_data
final_crawl_datalist <- rbind(final_crawl_datalist,final_crawl_data)
final_crawl_datalist
}
final_crawl_datalist
library("stringr")
library("mongolite")
#mongodb 저장하기
con <- mongo(collection = "crawl",
db = "bigdata",
url = "mongodb://127.0.0.1")
#크롤링데이터 불러오기
first_url <- "https://www.clien.net/service/group/community?&od=T31&po="
final_crawl_datalist <- NULL
for(i in 0:9){
final_url <- paste0(first_url,i)
final_url <- readLines(final_url,encoding = "UTF-8")
final_url
#데이터 필터링하기 - title
final_title <- final_url[str_detect(final_url,"subject_fixed")]
final_title <- str_extract(final_title,"(?<=\">).*(?=</span>)")
final_title
#데이터 필터링하기 - hit
final_hit <- final_url[str_detect(final_url,"<span class=\"hit\">")]
final_hit <- str_extract(final_hit,"(?<=\">).*(?=</span>)")[-1]
final_hit
#데이터 필터링하기 - url
filter_url <- final_url[which(str_detect(final_url,"subject_fixed"))-3]
filter_url
filter_url <- str_extract(filter_url,"(?<=href=\").*(?=data-role)")
filter_url
filter_url <- str_sub(filter_url,end = -3)#필요없는 문자열 잘라내기 : "end= -3"으로 뒤에서 3개를 잘라낸다.
filter_url
filter_url <- paste0("https://www.clien.net",filter_url)#사이트 주소 더하기
filter_url
#url을 이용해 내용 추출
contentlist <- NULL
filter_content <- NULL
for(page in 1:length(filter_url)){
contentdata <- readLines(filter_url[page],encoding = "UTF-8")
contentdata
start <- which(str_detect(contentdata,"post_content"))
end <- which(str_detect(contentdata,"post_ccls"))
filter_content <- contentdata[start:end]
filter_content
filter_content <- paste(filter_content,collapse = "")
filter_content <- gsub("<.*?>","",filter_content)#gsub(지울부분,지운부분 대체할내용,지울 데이터 이름)
filter_content <- gsub("\t|&nbsp;","",filter_content)
filter_content
#기존의 저장되어 있던 contentlist 내용에 추가
contentlist <- c(contentlist,filter_content)
contentlist
cat("\n",page)
final_crawl_data <- cbind(final_title,final_hit, filter_url,contentlist)
}
final_crawl_data
final_crawl_datalist <- rbind(final_crawl_datalist,final_crawl_data)
final_crawl_datalist
}
final_crawl_datalist
install.packages("N2H4")
library(N2H4)
install.packages("N2H4")
library(N2H4)
library(stringr)
library(dplyr)
install.packages("N2H4")
url <- "https://sports.news.naver.com/news.nhn?oid=477&aid=0000237139"
getAllComment(url) %>% select(userName, contents) <- mydata
mydata
library(N2H4)
library(stringr)
library(dplyr)
url <- "https://sports.news.naver.com/news.nhn?oid=477&aid=0000237139"
getAllComment(url) %>% select(userName, contents) <- mydata
mydata
#css의 선택자를 활용할 수 있는 방법 - rvest
install.packages("rvest")
install.packages("rvest")
library(rvest)
url <- "https://www.clien.net/service/group/community?&od=T31&po=0"
readPage <- read_html(url)
readPage
readPage %>% html_nodes("span.subject_fixed") <- title_data
title_data
url <- "https://www.clien.net/service/group/community?&od=T31&po=0"
readPage <- read_html(url)
readPage %>% html_nodes("span.subject_fixed") -> title_data
title_data
getAllComment(url) %>% select(userName, contents) -> mydata
mydata
url <- "https://sports.news.naver.com/news.nhn?oid=477&aid=0000237139"
getAllComment(url) %>% select(userName, contents) -> mydata
mydata
title_data
url <- "https://www.clien.net/service/group/community?&od=T31&po=0"
readPage <- read_html(url)
readPage %>% html_nodes("span.subject_fixed") -> title_data %>% html_text() -> title_data
title_data
readPage <- read_html(url)
readPage %>%
html_nodes("span.subject_fixed") -> title_data %>%
html_text() -> title_data
readPage %>%
html_nodes("span.subject_fixed")%>%
html_text() -> title_data
title_data
readPage %>%
html_nodes("span.subject_fixed") %>%
html_text() -> title_data
title_data
install.packages("KoNLP")
install.packages("KoNLP")
library(KoNLP)
install.packages("Sejong")
library(KoNLP)
install.packages("hash")
library(KoNLP)
install.packages("rJava")
library(KoNLP)
install.packages("tau")
library(KoNLP)
install.packages("RSQLite")
library(KoNLP)
install.packages("devtools")
library(KoNLP)
library(stringr)
####KoNLP 함수를 테스트####
extractNoun("롯데마트가 판매하고 있는 흑마늘 양념 치킨이 논란이 되고 있다.")
library(stringr)
useSejongDic()
####KoNLP 함수를 테스트####
extractNoun("롯데마트가 판매하고 있는 흑마늘 양념 치킨이 논란이 되고 있다.")
SimplePos09("롯데마트가 판매하고 있는 흑마늘 양념 치킨이 논란이 되고 있다.")
####분석할 샘플데이터 로딩####
load("comments.RData")
load("score.RData")
length(comments)
length(score)
head(comments,10)
head(score,10)
sampledata <- comments[1:1000]
class(sampledata)
for(i in 1:length(sampledata)){
data <- SimplePos09(sampledata[i])
data_list[[i]] <- data
}
data_list[[2]]
data_list <- list()#댓글의 길이는 전부 다르기 때문에 분리된 명사의 개수가 모두 다름 ==> list를 이용한다.
for(i in 1:length(sampledata)){
data <- SimplePos09(sampledata[i])
data_list[[i]] <- data
}
data_list[[2]]
data_list[[22]]
head(data_list,20)
#/로 분할하는 작업 ==> sapply를 이용해 반복작업을 할 수 있다.
sapply
#1. 리스트의 모든 요송 저장된 문자열을 /로 분할하는 작업 ==> sapply를 이용해 반복작업을 할 수 있다.
#2. 명사 N이 있는 문자열 가져오기 ==> "전화/N+하고/J"형태의 데이터이기 때문에 N앞에 있는 명사를 뽑는다.
sapply(data.frame(test=c(1,2,3,4,5,6),
test2=c(3,4,5,6,7,8)
),#반복작업할 데이터
function(x){
x[1]
}#반복해서 적용할 함수
)
#1. 리스트의 모든 요송 저장된 문자열을 /로 분할하는 작업 ==> sapply를 이용해 반복작업을 할 수 있다.
#2. 명사 N이 있는 문자열 가져오기 ==> "전화/N+하고/J"형태의 데이터이기 때문에 N앞에 있는 명사를 뽑는다.
#ex] sapply 예제
#     sapply(data.frame(test=c(1,2,3,4,5,6),
#                  test2=c(3,4,5,6,7,8)
#                ),#반복작업할 데이터
#       function(x){
#         x[1]
#       }#반복해서 적용할 함수
#       )
wordlist <- sapply(str_split(data_list,"/"), function(x){
x[1]
})
wordlist
head(wordlist,10)
head(data_list,20)
head(wordlist,10)
first_url <- "https://www.clien.net/service/group/community?&od=T31&po="
final_crawl_datalist <- NULL
for(i in 0:9){
final_url <- paste0(first_url,i)
final_url <- readLines(final_url,encoding = "UTF-8")
final_url
#데이터 필터링하기 - title
final_title <- final_url[str_detect(final_url,"subject_fixed")]
final_title
final_url <- paste0(first_url,i)
final_url <- readLines(final_url,encoding = "UTF-8")
final_url
#데이터 필터링하기 - title
final_title <- final_url[str_detect(final_url,"subject_fixed")]
final_title
for(i in 0:9){
final_url <- paste0(first_url,i)
final_url <- readLines(final_url,encoding = "UTF-8")
final_url
#데이터 필터링하기 - title
final_title <- final_url[str_detect(final_url,"subject_fixed")]
}final_title
for(i in 0:9){
final_url <- paste0(first_url,i)
final_url <- readLines(final_url,encoding = "UTF-8")
final_url
#데이터 필터링하기 - title
final_title <- final_url[str_detect(final_url,"subject_fixed")]
final_title}
final_title
final_title[2]
first_url <- "https://www.clien.net/service/group/community?&od=T31&po="
final_crawl_datalist <- NULL
for(i in 0:9){
final_url <- paste0(first_url,i)
final_url <- readLines(final_url,encoding = "UTF-8")
final_url
#데이터 필터링하기 - title
final_title <- final_url[str_detect(final_url,"subject_fixed")]
final_title <- str_extract(final_title,"(?<=\">).*(?=</span>)")
final_title
#데이터 필터링하기 - hit
final_hit <- final_url[str_detect(final_url,"<span class=\"hit\">")]
final_hit <- str_extract(final_hit,"(?<=\">).*(?=</span>)")[-1]
final_hit
#데이터 필터링하기 - url
filter_url <- final_url[which(str_detect(final_url,"subject_fixed"))-3]
filter_url
filter_url <- str_extract(filter_url,"(?<=href=\").*(?=data-role)")
filter_url
filter_url <- str_sub(filter_url,end = -3)#필요없는 문자열 잘라내기 : "end= -3"으로 뒤에서 3개를 잘라낸다.
filter_url
filter_url <- paste0("https://www.clien.net",filter_url)#사이트 주소 더하기
filter_url
#url을 이용해 내용 추출
contentlist <- NULL
filter_content <- NULL
for(page in 1:length(filter_url)){
contentdata <- readLines(filter_url[page],encoding = "UTF-8")
contentdata
start <- which(str_detect(contentdata,"post_content"))
end <- which(str_detect(contentdata,"post_ccls"))
filter_content <- contentdata[start:end]
filter_content
filter_content <- paste(filter_content,collapse = "")
filter_content <- gsub("<.*?>","",filter_content)#gsub(지울부분,지운부분 대체할내용,지울 데이터 이름)
filter_content <- gsub("\t|&nbsp;","",filter_content)
filter_content
#기존의 저장되어 있던 contentlist 내용에 추가
contentlist <- c(contentlist,filter_content)
contentlist
cat("\n",page)
final_crawl_data <- cbind(final_title,final_hit, filter_url,contentlist)
}
final_crawl_data
final_crawl_datalist <- rbind(final_crawl_datalist,final_crawl_data)
final_crawl_datalist
}
final_crawl_datalist
#저장하고 몽고DB 출력
write.csv(final_crawl_datalist,"crawl_result.csv")
save(final_crawl_datalist,file = "crawl_result.RData")
if(con$count()>0){
con$drop()
}
final_mongo_data <- data.frame(final_crawl_datalist)
con$insert(final_mongo_data)
class(final_mongo_data)
library("stringr")
library("mongolite")
#mongodb 저장하기
con <- mongo(collection = "crawl",
db = "bigdata",
url = "mongodb://127.0.0.1")
#크롤링데이터 불러오기
first_url <- "https://www.clien.net/service/group/community?&od=T31&po="
final_crawl_datalist <- NULL
for(i in 0:9){
final_url <- paste0(first_url,i)
final_url <- readLines(final_url,encoding = "UTF-8")
final_url
#데이터 필터링하기 - title
final_title <- final_url[str_detect(final_url,"subject_fixed")]
final_title <- str_extract(final_title,"(?<=\">).*(?=</span>)")
final_title
#데이터 필터링하기 - hit
final_hit <- final_url[str_detect(final_url,"<span class=\"hit\">")]
final_hit <- str_extract(final_hit,"(?<=\">).*(?=</span>)")[-1]
final_hit
#데이터 필터링하기 - url
filter_url <- final_url[which(str_detect(final_url,"subject_fixed"))-3]
filter_url
filter_url <- str_extract(filter_url,"(?<=href=\").*(?=data-role)")
filter_url
filter_url <- str_sub(filter_url,end = -3)#필요없는 문자열 잘라내기 : "end= -3"으로 뒤에서 3개를 잘라낸다.
filter_url
filter_url <- paste0("https://www.clien.net",filter_url)#사이트 주소 더하기
filter_url
#url을 이용해 내용 추출
contentlist <- NULL
filter_content <- NULL
for(page in 1:length(filter_url)){
contentdata <- readLines(filter_url[page],encoding = "UTF-8")
contentdata
start <- which(str_detect(contentdata,"post_content"))
end <- which(str_detect(contentdata,"post_ccls"))
filter_content <- contentdata[start:end]
filter_content
filter_content <- paste(filter_content,collapse = "")
filter_content <- gsub("<.*?>","",filter_content)#gsub(지울부분,지운부분 대체할내용,지울 데이터 이름)
filter_content <- gsub("\t|&nbsp;","",filter_content)
filter_content
#기존의 저장되어 있던 contentlist 내용에 추가
contentlist <- c(contentlist,filter_content)
contentlist
cat("\n",page)
final_crawl_data <- cbind(final_title,final_hit, filter_url,contentlist)
}
final_crawl_data
final_crawl_datalist <- rbind(final_crawl_datalist,final_crawl_data)
final_crawl_datalist
}
final_crawl_datalist
#저장하고 몽고DB 출력
write.csv(final_crawl_datalist,"crawl_result.csv")
save(final_crawl_datalist,file = "crawl_result.RData")
if(con$count()>0){
con$drop()
}
final_mongo_data <- data.frame(final_crawl_datalist)
con$insert(final_mongo_data)
class(final_mongo_data)
library(KoNLP)
library(stringr)
####KoNLP 함수를 테스트####
extractNoun("롯데마트가 판매하고 있는 흑마늘 양념 치킨이 논란이 되고 있다.")
SimplePos09("롯데마트가 판매하고 있는 흑마늘 양념 치킨이 논란이 되고 있다.")
####분석할 샘플데이터 로딩####
load("comments.RData")
load("score.RData")
length(comments)
length(score)
head(comments,10)
head(score,10)
sampledata <- comments[1:1000]
class(sampledata)
####형태소분석을 하기 위한 명사 분리####
data_list <- list()#댓글의 길이는 전부 다르기 때문에 분리된 명사의 개수가 모두 다름 ==> list를 이용한다.
for(i in 1:length(sampledata)){
data <- SimplePos09(sampledata[i])
data_list[[i]] <- data
}
data_list[[22]]
head(data_list,20)
wordlist <- sapply(str_split(data_list,"/"), function(x){
x[1]
})
data_list[[22]]
head(data_list,20)
#1. 리스트의 모든 요송 저장된 문자열을 /로 분할하는 작업 ==> sapply를 이용해 반복작업을 할 수 있다.
#2. 명사 N이 있는 문자열 가져오기 ==> "전화/N+하고/J"형태의 데이터이기 때문에 N앞에 있는 명사를 뽑는다.
#ex] sapply 예제
#     sapply(data.frame(test=c(1,2,3,4,5,6),
#                  test2=c(3,4,5,6,7,8)
#                ),#반복작업할 데이터
#       function(x){
#         x[1]
#       }#반복해서 적용할 함수
#       )
wordlist <- sapply(str_split(data_list,"/"), function(x){
x[1]
})
library(KoNLP)
library(stringr)
wordlist <- sapply(str_split(data_list,"/"), function(x){
x[1]
})
wordlist
wordlist
head(wordlist,10)
#2. 명사 N이 있는 문자열 가져오기 ==> "전화/N+하고/J"형태의 데이터이기 때문에 N앞에 있는 명사를 뽑는다.
#ex] sapply 예제
#     sapply(data.frame(test=c(1,2,3,4,5,6),
#                  test2=c(3,4,5,6,7,8)
#                ),#반복작업할 데이터
#       function(x){
#         x[1]
#       }#반복해서 적용할 함수
#       )
#리스트를
class(unlist(data_list))
class(wordlist)
class(unlist(data_list))
wordlist <- sapply(str_split(unlist(data_list),"/"), function(x){
x[1]
})
class(wordlist)
head(wordlist,10)
#table함수를 이용해서 단어의 빈도수를 구하기
table(wordlist)
#table함수를 이용해서 단어의 빈도수를 구하기
tablewordlist <- table(wordlist)
tablewordlist[1]
tablewordlist[88]
tablewordlist[89]
tablewordlist[100]
tablewordlist[88]
names(tablewordlist)
class(unlist(data_list))
wordlist <- sapply(str_split(unlist(data_list),"/"), function(x){
x[1]
})
class(wordlist)
head(wordlist,10)
#table함수를 이용해서 단어의 빈도수를 구하기
#table함수 : 벡터에 저장되어 있는 모든 단어들의 빈도수를 계산하여 변환
#             단어를 변수명으로 사용
tablewordlist <- table(wordlist)
tablewordlist[1]
tablewordlist[2]
names(tablewordlist)
names(tablewordlist,head)
names(tablewordlist,n=2)
tablewordlist[1]
tablewordlist[89]
sort(tablewordlist,decreasing = T)[1:100]
nchar("글자수")
tablewordlist_result <- tablewordlist[nchar(names(tablewordlist))>1]
tablewordlist_result <- sort(tablewordlist_result,decreasing = T)[1:100]
tablewordlist_result
tablewordlist[89]
#분석한 데이터를 sort
sort(tablewordlist,decreasing = T)[1:100]
#table함수를 이용해서 단어의 빈도수를 구하기
#table함수 : 벡터에 저장되어 있는 모든 단어들의 빈도수를 계산하여 변환
#             단어를 변수명으로 사용
tablewordlist <- table(wordlist)
tablewordlist[1]
tablewordlist[]]
tablewordlist[]
install.packages("RColorBrewer")
install.packages("wordcloud")
library(wordcloud)
library(RColorBrewer)
head(wordlist,10)
#table함수를 이용해서 단어의 빈도수를 구하기
#table함수 : 벡터에 저장되어 있는 모든 단어들의 빈도수를 계산하여 변환
#             단어를 변수명으로 사용
tablewordlist <- table(wordlist)
tablewordlist
#분석한 데이터를 sort
sort(tablewordlist,decreasing = T)[1:100]
#분석한 데이터를 sort
sort(tablewordlist,decreasing = T)
#분석한 데이터를 sort
sort(tablewordlist,decreasing = T)[1:100]
brewer.pal.info
display.brewer.all(type="div")
display.brewer.all(type="div")
display.brewer.all(type="qual")
display.brewer.all(type="seq")
display.brewer.all(type="div")
display.brewer.all(type="qual")
display.brewer.all(type="seq")
word <- tablewordlist_result
word <- names(tablewordlist_result)
word <- names(tablewordlist_result)
word
count <- as.numeric(tablewordlist_result)
count
brewer.pal.info
class(tablewordlist_result)
mycolor <- brewer.pal(n=11,name="RdYlBu")
wordcloud(words = word,freq = count,random.order = F,colors = mycolor,scale = c(7,0.3))
wordcloud(words = word,freq = count,random.order = F,colors = mycolor,scale = c(7,0.5))
wordcloud(words = word,freq = count,random.order = F,colors = mycolor,scale = c(7,1))
wordcloud(words = word,freq = count,random.order = F,colors = mycolor,scale = c(6,1))
wordcloud(words = word,freq = count,random.order = F,colors = mycolor,scale = c(5,1))
wordcloud(words = word,freq = count,random.order = T,colors = mycolor,scale = c(5,1))
wordcloud(words = word,freq = count,random.order = F,colors = mycolor,scale = c(5,1))
mycolor <- brewer.pal(n=100,name="RdYlBu")
wordcloud(words = word,freq = count,random.order = F,colors = mycolor,scale = c(5,1))
mycolor <- brewer.pal(n=5,name="RdYlBu")
wordcloud(words = word,freq = count,random.order = F,colors = mycolor,scale = c(5,1))
mycolor <- brewer.pal(n=8,name="RdYlBu")
wordcloud(words = word,freq = count,random.order = F,colors = mycolor,scale = c(5,1))
class(wordlist)
#2. 명사 N이 있는 문자열 가져오기 ==> "전화/N+하고/J"형태의 데이터이기 때문에 N앞에 있는 명사를 뽑는다.
#ex] sapply 예제
#     sapply(data.frame(test=c(1,2,3,4,5,6),
#                  test2=c(3,4,5,6,7,8)
#                ),#반복작업할 데이터
#       function(x){
#         x[1]
#       }#반복해서 적용할 함수
#       )
#리스트를 unlist를 사용하여 character로 변환하여 출력한다.
class(unlist(data_list))
